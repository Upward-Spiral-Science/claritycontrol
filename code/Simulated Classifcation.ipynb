{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Classifcation\n",
    "\n",
    "1. State assumptions\n",
    "2. Formally define classification/regression problem\n",
    "3. provide algorithm for solving problem (including choosing hyperparameters as appropriate)\n",
    "4. sample data from a simulation setting inspired by your data (from both null and alternative as defined before)\n",
    "5. compute accuracy\n",
    "6. plot accuracy vs. sample size in simulation\n",
    "7. apply method directly on real data\n",
    "8. explain the degree to which you believe the result and why\n",
    "\n",
    "## Step 1: State assumptions\n",
    "\n",
    "\n",
    "\n",
    "## Step 2: Formally define classification/regression problem\n",
    "\n",
    "## Step 3: Provide algorithm for solving problem (including choosing hyperparameters as appropriate)\n",
    "\n",
    "\"Machine Learning\" is usually classification or prediction\n",
    "  - predictive is subject specific\n",
    "  \n",
    "Clarity brains have $(X, Y)$ ~iid $F_{xy}$ which is some distribution\n",
    "  - X is subject \n",
    "  - Y is $\\{0, 1\\}$\n",
    "  - Function g(x) spits out a class label (thus g is a classifier function)\n",
    "  - G = {g: map from reals to {0, 1}}\n",
    "    - Classifier takes a single x \n",
    "      - If $x>k$ but $<0$ is one classifier\n",
    "      - Best clasisifier is statistical decision theory\n",
    "        - Need to define a loss function that tells us how wrong we are\n",
    "        - We need to choose classifier that minimizes loss\n",
    "        - G* = $\\underset{g \\in G}{argmin} l(g(x), y)$\n",
    "        - Squared error is a good option $(g(x)-y)^2$\n",
    "          - Problem is that $(0-1)^2 = (1-0)^2$ so you don't know which side of the \"wrong\" you are\n",
    "        - Absolute error is $|g(x)-y|$\n",
    "        - Zero one error \n",
    "          - If $g(x)=y$ then $l=0$\n",
    "          - If $g(y)!=y$ then $l=1$\n",
    "    - If L is the set of loss functions \n",
    "      - L = {l: yxy -> Real+}\n",
    "        - Here we are finding which scores are the best \n",
    "    - Definitions: Voxels, Priors, Baye's rule F_(x,y) = F_(x|y)F_y=F_(y|x)F_x ->  \n",
    "      - $F_(x|y)=N(M_y. 1)$\n",
    "      - $F_y = Bern(pi)$\n",
    "      - Next need to fit the joint distribution \n",
    "        - After fitting the Bayes optimal is called the Bayes plugin\n",
    "        - MLE - minimizing squared error\n",
    "        - Sample n train sample (xi, yi) ~iid F_xx generate training data i∈[n_train]\n",
    "        - estimate classifier theta\n",
    "          - sample iidF_xy i∈[n_test]\n",
    "          \n",
    "    \n",
    "    - Best classifier is called the Bayes Optimal\n",
    "      - g* = argmax F_((x=(x)|(y)=y))\n",
    "        - Can use a posterari if priors are not equal\n",
    "          - F_(x=x, y=y) = F_(x|y)F_y\n",
    "        - compute argmax for y∈y\n",
    "          - Let y = 0 is .99 y-1 is .01\n",
    "        \n",
    "    - Next need to relect get change level accuracy which will almost definitely ahppen if you use a regular loss function \n",
    "      - Use histogram instead of image data\n",
    "        - Classifier list:\n",
    "          - LDA\n",
    "            - Variances are the same \n",
    "            - Made by Fischer\n",
    "            - Finds optimal linear classifier (optimal line) under the assumptions that we have made\n",
    "              - Advantages: Very interpretable, Very fast, Linear\n",
    "          - Random Forest\n",
    "            - Decision tree thresholds are created\n",
    "            - Choose a loss function and then try to do a greedy search\n",
    "            - Find the optimal thresholds to maximize purity\n",
    "            - Change thresholds to maximize purity so that most of one group is in one part and the others are in the others\n",
    "            - Random Forest uses decision trees on subsets of your data, since each tree is noisy and can overfit, so averageing over many different classifiers it will be much more effective\n",
    "            - This is an ensemble method \n",
    "              - Every single classifier is on a different point on the bias variance tradeoff so when you average everything it will be more consistent\n",
    "          - SVM\n",
    "          - Logistic\n",
    "          - Neural Network\n",
    "            - Uses linear algebra, runs on GPU\n",
    "            - Takes in more information and is very useful for computer vision techniques\n",
    "            - Natively do the classificiation\n",
    "          - KNN\n",
    "            - K nearest neighbor \n",
    "            - specify apriori k and find the distance between the points and K \n",
    "            - Assuming K is big enough, it will always converge irrespectively\n",
    "            - Doesn't care about the distributions, but it is universally consistent\n",
    "          - QDA\n",
    "            - Quadratic descriminatory analysis\n",
    "            - Optimal discriminatory boundary is curved\n",
    "            - Covariance matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change working dir\n",
    "path = \"/Users/david/Desktop/CourseWork/TheArtOfDataScience/claritycontrol/code/scripts\" # use your own path\n",
    "import os\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram data preparation\n",
    "\n",
    "> Skip this step if the data are already baked, just eat them!\n",
    "\n",
    "1. Set suitable data value range to get histogram from the majority of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import clarity.resources as rs\n",
    "import clarity as cl\n",
    "import gc  # garbage memory collection :)\n",
    "\n",
    "BINS=4\n",
    "RANGE=(10.0,300.0)\n",
    "\n",
    "# For testing, find the proper range.\n",
    "# import matplotlib.pyplot as plt\n",
    "# c = cl.Clarity(\"Cocaine178\")\n",
    "# hist, bin_edges = c.loadImg(info=False).getHistogram(bins=BINS,range=RANGE,density=True)\n",
    "# width = 0.8*(bin_edges[1] - bin_edges[0])\n",
    "# center = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "# plt.bar(center, hist, align='center', width=width)\n",
    "# plt.show()\n",
    "\n",
    "for token in rs.TOKENS:\n",
    "    c = cl.Clarity(token)\n",
    "    hist, bin_edges = c.loadImg(info=False).getHistogram(bins=BINS,range=RANGE,density=True)\n",
    "    fname = \"../data/hist/\"+token+\".hist\"\n",
    "    np.savetxt(fname,hist,delimiter=',')\n",
    "    # print fname,\"saved.\"\n",
    "    del c\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import jgraph as ig\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12345678)  # for reproducibility, set random seed\n",
    "mu = [1,2,3] # TODO: use average from data\n",
    "std = [1,1,1] # TODO: use deviation from data\n",
    "\n",
    "# define number of subjects per class\n",
    "S = np.array((9, 21, 30, 39, 45, 63, 81, 96, 108, 210, 333,\n",
    "              666, 801, 999))\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Random Forest\",\n",
    "         \"Linear Discriminant Analysis\", \"Quadratic Discriminant Analysis\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3), # TODO: what is 3\n",
    "    SVC(kernel=\"linear\", C=0.5), # TODO: what is C\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    LinearDiscriminantAnalysis()]\n",
    "#     QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 4 & 5: Sample data from setting similar to data and record classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Nearest Neighbors: 0.56 (+/- 0.99)\n",
      "Accuracy of Linear SVM: 0.78 (+/- 0.83)\n",
      "Accuracy of Random Forest: 0.67 (+/- 0.94)\n",
      "Accuracy of Linear Discriminant Analysis: 0.56 (+/- 0.99)\n",
      "Accuracy of Nearest Neighbors: 0.76 (+/- 0.85)\n",
      "Accuracy of Linear SVM: 0.90 (+/- 0.59)\n",
      "Accuracy of Random Forest: 0.76 (+/- 0.85)\n",
      "Accuracy of Linear Discriminant Analysis: 0.76 (+/- 0.85)\n",
      "Accuracy of Nearest Neighbors: 0.63 (+/- 0.96)\n",
      "Accuracy of Linear SVM: 0.77 (+/- 0.85)\n",
      "Accuracy of Random Forest: 0.60 (+/- 0.98)\n",
      "Accuracy of Linear Discriminant Analysis: 0.77 (+/- 0.85)\n",
      "Accuracy of Nearest Neighbors: 0.69 (+/- 0.92)\n",
      "Accuracy of Linear SVM: 0.77 (+/- 0.84)\n",
      "Accuracy of Random Forest: 0.62 (+/- 0.97)\n",
      "Accuracy of Linear Discriminant Analysis: 0.79 (+/- 0.81)\n",
      "Accuracy of Nearest Neighbors: 0.76 (+/- 0.86)\n",
      "Accuracy of Linear SVM: 0.82 (+/- 0.76)\n",
      "Accuracy of Random Forest: 0.69 (+/- 0.93)\n",
      "Accuracy of Linear Discriminant Analysis: 0.82 (+/- 0.76)\n",
      "Accuracy of Nearest Neighbors: 0.65 (+/- 0.95)\n",
      "Accuracy of Linear SVM: 0.71 (+/- 0.90)\n",
      "Accuracy of Random Forest: 0.70 (+/- 0.92)\n",
      "Accuracy of Linear Discriminant Analysis: 0.68 (+/- 0.93)\n",
      "Accuracy of Nearest Neighbors: 0.65 (+/- 0.95)\n",
      "Accuracy of Linear SVM: 0.67 (+/- 0.94)\n",
      "Accuracy of Random Forest: 0.64 (+/- 0.96)\n",
      "Accuracy of Linear Discriminant Analysis: 0.73 (+/- 0.89)\n",
      "Accuracy of Nearest Neighbors: 0.70 (+/- 0.92)\n",
      "Accuracy of Linear SVM: 0.70 (+/- 0.92)\n",
      "Accuracy of Random Forest: 0.71 (+/- 0.91)\n",
      "Accuracy of Linear Discriminant Analysis: 0.74 (+/- 0.88)\n",
      "Accuracy of Nearest Neighbors: 0.77 (+/- 0.84)\n",
      "Accuracy of Linear SVM: 0.83 (+/- 0.75)\n",
      "Accuracy of Random Forest: 0.76 (+/- 0.86)\n",
      "Accuracy of Linear Discriminant Analysis: 0.83 (+/- 0.75)\n",
      "Accuracy of Nearest Neighbors: 0.71 (+/- 0.90)\n",
      "Accuracy of Linear SVM: 0.80 (+/- 0.80)\n",
      "Accuracy of Random Forest: 0.71 (+/- 0.90)\n",
      "Accuracy of Linear Discriminant Analysis: 0.79 (+/- 0.81)\n",
      "Accuracy of Nearest Neighbors: 0.72 (+/- 0.89)\n",
      "Accuracy of Linear SVM: 0.77 (+/- 0.84)\n",
      "Accuracy of Random Forest: 0.72 (+/- 0.90)\n",
      "Accuracy of Linear Discriminant Analysis: 0.76 (+/- 0.86)\n",
      "Accuracy of Nearest Neighbors: 0.72 (+/- 0.90)\n",
      "Accuracy of Linear SVM: 0.77 (+/- 0.85)\n",
      "Accuracy of Random Forest: 0.74 (+/- 0.88)\n",
      "Accuracy of Linear Discriminant Analysis: 0.77 (+/- 0.85)\n",
      "Accuracy of Nearest Neighbors: 0.73 (+/- 0.89)\n",
      "Accuracy of Linear SVM: 0.80 (+/- 0.80)\n",
      "Accuracy of Random Forest: 0.74 (+/- 0.88)\n",
      "Accuracy of Linear Discriminant Analysis: 0.79 (+/- 0.81)\n",
      "Accuracy of Nearest Neighbors: 0.74 (+/- 0.88)\n",
      "Accuracy of Linear SVM: 0.79 (+/- 0.82)\n",
      "Accuracy of Random Forest: 0.74 (+/- 0.88)\n",
      "Accuracy of Linear Discriminant Analysis: 0.79 (+/- 0.81)\n",
      "[[[ 0.55555556  0.49690399]\n",
      "  [ 0.77777778  0.41573971]\n",
      "  [ 0.66666667  0.47140452]\n",
      "  [ 0.55555556  0.49690399]]\n",
      "\n",
      " [[ 0.76190476  0.42591771]\n",
      "  [ 0.9047619   0.29354352]\n",
      "  [ 0.76190476  0.42591771]\n",
      "  [ 0.76190476  0.42591771]]\n",
      "\n",
      " [[ 0.63333333  0.48189441]\n",
      "  [ 0.76666667  0.42295258]\n",
      "  [ 0.6         0.48989795]\n",
      "  [ 0.76666667  0.42295258]]\n",
      "\n",
      " [[ 0.69230769  0.46153846]\n",
      "  [ 0.76923077  0.42132504]\n",
      "  [ 0.61538462  0.48650426]\n",
      "  [ 0.79487179  0.40379528]]\n",
      "\n",
      " [[ 0.75555556  0.42975732]\n",
      "  [ 0.82222222  0.38232557]\n",
      "  [ 0.68888889  0.46294815]\n",
      "  [ 0.82222222  0.38232557]]\n",
      "\n",
      " [[ 0.65079365  0.47671928]\n",
      "  [ 0.71428571  0.45175395]\n",
      "  [ 0.6984127   0.45894706]\n",
      "  [ 0.68253968  0.4654882 ]]\n",
      "\n",
      " [[ 0.65432099  0.47558914]\n",
      "  [ 0.66666667  0.47140452]\n",
      "  [ 0.64197531  0.47941945]\n",
      "  [ 0.72839506  0.44478725]]\n",
      "\n",
      " [[ 0.69791667  0.45916118]\n",
      "  [ 0.69791667  0.45916118]\n",
      "  [ 0.70833333  0.45452967]\n",
      "  [ 0.73958333  0.43886197]]\n",
      "\n",
      " [[ 0.76851852  0.42177933]\n",
      "  [ 0.83333333  0.372678  ]\n",
      "  [ 0.75925926  0.4275332 ]\n",
      "  [ 0.83333333  0.372678  ]]\n",
      "\n",
      " [[ 0.71428571  0.45175395]\n",
      "  [ 0.8         0.4       ]\n",
      "  [ 0.71428571  0.45175395]\n",
      "  [ 0.79047619  0.40696877]]\n",
      "\n",
      " [[ 0.72372372  0.44715511]\n",
      "  [ 0.77177177  0.41969049]\n",
      "  [ 0.71771772  0.45010998]\n",
      "  [ 0.75675676  0.42904075]]\n",
      "\n",
      " [[ 0.71621622  0.45083317]\n",
      "  [ 0.76576577  0.42351925]\n",
      "  [ 0.73723724  0.44013463]\n",
      "  [ 0.76726727  0.42257332]]\n",
      "\n",
      " [[ 0.73158552  0.44313446]\n",
      "  [ 0.79775281  0.40167557]\n",
      "  [ 0.73907615  0.43913847]\n",
      "  [ 0.79026217  0.40712145]]\n",
      "\n",
      " [[ 0.73873874  0.43932199]\n",
      "  [ 0.78578579  0.41027611]\n",
      "  [ 0.74174174  0.43767674]\n",
      "  [ 0.79379379  0.40458028]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:453: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.zeros((len(S), len(classifiers), 2), dtype=np.dtype('float64'))\n",
    "for idx1, s in enumerate(S):\n",
    "    s0=s/3\n",
    "    s1=s/3\n",
    "    s2=s/3\n",
    "    \n",
    "    x0 = np.random.normal(mu[0],std[0],(s0,4))\n",
    "    x1 = np.random.normal(mu[1],std[1],(s1,4))\n",
    "    x2 = np.random.normal(mu[2],std[2],(s2,4))\n",
    "    X = x0\n",
    "    X = np.vstack([X,x1])\n",
    "    X = np.vstack([X,x2])\n",
    "    y = np.append(np.append(np.zeros(s0), np.ones(s1)),np.ones(s2)*2)\n",
    "    \n",
    "    for idx2, cla in enumerate(classifiers):\n",
    "        X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "        clf = cla.fit(X_train, y_train)\n",
    "        loo = LeaveOneOut(len(X))\n",
    "        scores = cross_validation.cross_val_score(clf, X, y, cv=loo)\n",
    "        accuracy[idx1, idx2,] = [scores.mean(), scores.std()]\n",
    "        print(\"Accuracy of %s: %0.2f (+/- %0.2f)\" % (names[idx2], scores.mean(), scores.std() * 2))\n",
    "    \n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Plot Accuracy versus N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.errorbar(S, accuracy[:,0,0], yerr = accuracy[:,0,1], hold=True, label=names[0])\n",
    "plt.errorbar(S, accuracy[:,1,0], yerr = accuracy[:,1,1], color='green', hold=True, label=names[1])\n",
    "plt.errorbar(S, accuracy[:,2,0], yerr = accuracy[:,2,1], color='red', hold=True, label=names[2])\n",
    "plt.errorbar(S, accuracy[:,3,0], yerr = accuracy[:,3,1], color='black', hold=True, label=names[3])\n",
    "plt.errorbar(S, accuracy[:,4,0], yerr = accuracy[:,4,1], color='brown', hold=True, label=names[4])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('number of samples')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracty of gender classification under simulated data')\n",
    "plt.axhline(1, color='red', linestyle='--')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
